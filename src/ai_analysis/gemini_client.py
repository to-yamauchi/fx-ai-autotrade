"""
========================================
Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
========================================

ãƒ•ã‚¡ã‚¤ãƒ«å: gemini_client.py
ãƒ‘ã‚¹: src/ai_analysis/gemini_client.py

ã€æ¦‚è¦ã€‘
Google Gemini APIã¨é€£æºã—ã€ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‰åˆ¤æ–­ã‚’è¡Œã†
ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã™ã€‚3ã¤ã®ãƒ¢ãƒ‡ãƒ«(Pro/Flash/Flash-8B)ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€
æŸ”è»Ÿã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã€ä¸»ãªæ©Ÿèƒ½ã€‘
1. Gemini APIé€£æº
2. ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
3. AIåˆ¤æ–­çµæœã®ãƒ‘ãƒ¼ã‚¹
4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

ã€ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ã€‘
ãƒ¢ãƒ‡ãƒ«åã¯.envãƒ•ã‚¡ã‚¤ãƒ«ã§è¨­å®šå¯èƒ½:
- GEMINI_MODEL_DAILY_ANALYSIS: ãƒ‡ã‚¤ãƒªãƒ¼åˆ†æç”¨ï¼ˆPhase 1, 2, 5ï¼‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.5-flashï¼‰
- GEMINI_MODEL_PERIODIC_UPDATE: å®šæœŸæ›´æ–°ç”¨ï¼ˆPhase 3ï¼‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.5-flashï¼‰
- GEMINI_MODEL_POSITION_MONITOR: ãƒã‚¸ã‚·ãƒ§ãƒ³ç›£è¦–ç”¨ï¼ˆPhase 4ï¼‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.5-flashï¼‰

æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§: https://ai.google.dev/gemini-api/docs/models

ã€å‡ºåŠ›å½¢å¼ã€‘
{
    "action": "BUY/SELL/HOLD",
    "confidence": 0-100,
    "reasoning": "åˆ¤æ–­ç†ç”±",
    "entry_price": ã‚¨ãƒ³ãƒˆãƒªãƒ¼ä¾¡æ ¼,
    "stop_loss": SLä¾¡æ ¼,
    "take_profit": TPä¾¡æ ¼
}

ã€ä½œæˆæ—¥ã€‘2025-10-22
"""

import google.generativeai as genai
from typing import Dict, Optional
import os
import logging
import json
import re
import time
from google.api_core import exceptions as google_exceptions
from src.ai_analysis.base_llm_client import BaseLLMClient


class GeminiClient(BaseLLMClient):
    """
    Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒ©ã‚¹

    ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‰åˆ¤æ–­ã‚’è¡Œã†ãŸã‚ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã€‚
    è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€ãƒ¢ãƒ‡ãƒ«é¸æŠã«ã‚ˆã‚Šç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´å¯èƒ½ã€‚
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        GeminiClientã®åˆæœŸåŒ–

        Args:
            api_key: Gemini APIã‚­ãƒ¼ï¼ˆçœç•¥æ™‚ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰

        Raises:
            ValueError: GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
        """
        from src.utils.config import get_config

        # .envã‹ã‚‰è¨­å®šã‚’å¼·åˆ¶çš„ã«èª­ã¿è¾¼ã¿
        self.config = get_config()

        # APIã‚­ãƒ¼ã®å–å¾—ï¼ˆå¼•æ•°å„ªå…ˆã€æ¬¡ã«ç’°å¢ƒå¤‰æ•°ï¼‰
        if api_key is None:
            api_key = self.config.gemini_api_key

        if not api_key:
            raise ValueError("GEMINI_API_KEY environment variable is not set")

        # åŸºåº•ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        super().__init__(api_key)

        # Gemini APIã®è¨­å®š
        genai.configure(api_key=api_key)

        self.logger.info("âœ“ Gemini API initialized")

    def analyze_market(self,
                      market_data: Dict,
                      model: str = 'flash') -> Dict:
        """
        ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‰åˆ¤æ–­ã‚’è¡Œã†

        Args:
            market_data: æ¨™æº–åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆDataStandardizerã®å‡ºåŠ›ï¼‰
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« ('pro' / 'flash' / 'flash-lite')

        Returns:
            AIåˆ¤æ–­çµæœã®è¾æ›¸
            {
                'action': 'BUY' | 'SELL' | 'HOLD',
                'confidence': 0-100ã®æ•°å€¤,
                'reasoning': 'åˆ¤æ–­ç†ç”±ã®èª¬æ˜',
                'entry_price': ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¨å¥¨ä¾¡æ ¼ (optional),
                'stop_loss': SLæ¨å¥¨ä¾¡æ ¼ (optional),
                'take_profit': TPæ¨å¥¨ä¾¡æ ¼ (optional)
            }

        Raises:
            Exception: APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼æ™‚ï¼ˆã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«è¨˜éŒ²ã—ã€HOLDã‚’è¿”ã™ï¼‰
        """
        # åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        prompt = self._build_analysis_prompt(market_data)

        # ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
        selected_model, actual_model_name = self._select_model(model)

        try:
            # AIåˆ†æã®å®Ÿè¡Œï¼ˆãƒ­ã‚°ã¯æœ€å°é™ã«ï¼‰
            response = selected_model.generate_content(prompt)

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‘ãƒ¼ã‚¹
            result = self._parse_response(response.text)

            return result

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯HOLDã‚’è¿”ã™
            self.logger.error(f"âŒ AI analysis error: {e}")
            return {
                'action': 'HOLD',
                'confidence': 0,
                'reasoning': f'Error occurred during AI analysis: {str(e)}'
            }

    def generate_response(
        self,
        prompt: str,
        model: str = 'flash',
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        æ±ç”¨çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦AIå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹

        Args:
            prompt: AIã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« ('pro' / 'flash' / 'flash-8b')
            temperature: å¿œç­”ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ï¼ˆ0.0-1.0ï¼‰ã€Noneã®å ´åˆã¯.envã®è¨­å®šã‚’ä½¿ç”¨
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€Noneã®å ´åˆã¯.envã®è¨­å®šã‚’ä½¿ç”¨

        Returns:
            AIã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ

        Raises:
            Exception: APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼æ™‚
        """
        # ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
        selected_model, actual_model_name = self._select_model(model)

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šã‹ã‚‰å–å¾—
        if temperature is None:
            if model == 'pro' or model == 'daily_analysis':
                temperature = self.config.ai_temperature_daily_analysis
            elif model == 'flash-8b' or model == 'flash-lite' or model == 'position_monitor':
                temperature = self.config.ai_temperature_position_monitor
            else:  # flash or periodic_update
                temperature = self.config.ai_temperature_periodic_update

        if max_tokens is None:
            if model == 'pro' or model == 'daily_analysis':
                max_tokens = self.config.ai_max_tokens_daily_analysis
            elif model == 'flash-8b' or model == 'flash-lite' or model == 'position_monitor':
                max_tokens = self.config.ai_max_tokens_position_monitor
            else:  # flash or periodic_update
                max_tokens = self.config.ai_max_tokens_periodic_update

        try:
            # ç”Ÿæˆè¨­å®š
            generation_config = {
                'temperature': temperature,
            }
            # max_tokensãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿è¿½åŠ ï¼ˆNoneã®å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ï¼‰
            if max_tokens is not None:
                generation_config['max_output_tokens'] = max_tokens

            # AIå¿œç­”ã®ç”Ÿæˆï¼ˆãƒªãƒˆãƒ©ã‚¤å‡¦ç†ä»˜ãï¼‰
            max_retries = 3
            retry_delay = 2  # åˆå›å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

            for attempt in range(max_retries):
                try:
                    response = selected_model.generate_content(
                        prompt,
                        generation_config=generation_config
                    )
                    break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹

                except (google_exceptions.InternalServerError, google_exceptions.ResourceExhausted) as e:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•: 2ç§’ã€4ç§’ã€8ç§’
                        self.logger.warning(
                            f"Gemini API error (attempt {attempt + 1}/{max_retries}): {e}. "
                            f"Retrying in {wait_time} seconds..."
                        )
                        time.sleep(wait_time)
                    else:
                        # æœ€å¾Œã®ãƒªãƒˆãƒ©ã‚¤ã‚‚å¤±æ•—
                        self.logger.error(f"Gemini API failed after {max_retries} attempts: {e}")
                        raise

            # finish_reasonã‚’ãƒã‚§ãƒƒã‚¯
            if not response.parts:
                # responseã«partsãŒãªã„å ´åˆã¯finish_reasonã‚’ç¢ºèª
                finish_reason = response.candidates[0].finish_reason if response.candidates else None

                if finish_reason == 2:  # MAX_TOKENS
                    error_msg = (
                        "AIå¿œç­”ãŒæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«é”ã—ã¾ã—ãŸã€‚"
                        f"ç¾åœ¨ã®è¨­å®š: {max_tokens} tokensã€‚"
                        ".envã®max_tokensè¨­å®šã‚’å¢—ã‚„ã™ã‹ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çŸ­ãã—ã¦ãã ã•ã„ã€‚"
                    )
                    self.logger.error(f"âŒ {error_msg}")
                    raise ValueError(error_msg)
                elif finish_reason == 3:  # SAFETY
                    error_msg = (
                        "AIå¿œç­”ãŒå®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ã«ã‚ˆã‚Šãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚"
                        "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                    )
                    self.logger.error(f"âŒ {error_msg}")
                    raise ValueError(error_msg)
                else:
                    error_msg = f"AIå¿œç­”ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚finish_reason: {finish_reason}"
                    self.logger.error(f"âŒ {error_msg}")
                    raise ValueError(error_msg)

            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²
            if hasattr(response, 'usage_metadata'):
                from src.ai_analysis.token_usage_tracker import get_token_tracker
                tracker = get_token_tracker()
                input_tokens = response.usage_metadata.prompt_token_count
                output_tokens = response.usage_metadata.candidates_token_count
                tracker.record_usage(
                    phase=kwargs.get('phase', 'Unknown'),
                    provider='gemini',
                    model=actual_model_name,  # å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã‚’è¨˜éŒ²
                    input_tokens=input_tokens,
                    output_tokens=output_tokens
                )

            return response.text

        except Exception as e:
            self.logger.error(f"âŒ Generate response error: {e}")
            raise

    def _build_analysis_prompt(self, market_data: Dict) -> str:
        """
        åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹

        ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§æ•´å½¢ã—ã€AIã«åˆ†æã‚’ä¾é ¼ã™ã‚‹
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            market_data: æ¨™æº–åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿

        Returns:
            åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """
        # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
        market_data_json = json.dumps(market_data, indent=2, ensure_ascii=False)

        prompt = f"""ã‚ãªãŸã¯ãƒ—ãƒ­ã®FXãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‰åˆ¤æ–­ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

## ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
{market_data_json}

## åˆ†ææŒ‡ç¤º
1. **å„æ™‚é–“è¶³ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ**
   - D1ï¼ˆæ—¥è¶³ï¼‰: é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¢ºèª
   - H4ï¼ˆ4æ™‚é–“è¶³ï¼‰: ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¢ºèª
   - H1ï¼ˆ1æ™‚é–“è¶³ï¼‰: çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¢ºèª
   - M15ï¼ˆ15åˆ†è¶³ï¼‰: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’ç¢ºèª

2. **ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™åˆ†æ**
   - EMA: ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ã®ç¢ºèªï¼ˆçŸ­æœŸEMAã¨é•·æœŸEMAã®é–¢ä¿‚ï¼‰
   - RSI: è²·ã‚ã‚Œã™ã/å£²ã‚‰ã‚Œã™ãã®åˆ¤æ–­
   - MACD: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã¨ãƒˆãƒ¬ãƒ³ãƒ‰è»¢æ›ã®ç¢ºèª
   - Bollinger Bands: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨ä¾¡æ ¼ä½ç½®ã®ç¢ºèª
   - ATR: ç¾åœ¨ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«

3. **ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹**
   - é‡è¦ãªä¾¡æ ¼ãƒ¬ãƒ™ãƒ«ã‚’è€ƒæ…®
   - ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã®å¯èƒ½æ€§ã‚’è©•ä¾¡

4. **ç·åˆåˆ¤æ–­**
   - ä¸Šè¨˜ã®åˆ†æã‚’ç·åˆã—ã€BUY/SELL/HOLDã®ã„ãšã‚Œã‹ã‚’é¸æŠ
   - ä¿¡é ¼åº¦ï¼ˆ0-100ï¼‰ã‚’æ•°å€¤ã§ç¤ºã™
   - åˆ¤æ–­ç†ç”±ã‚’æ˜ç¢ºã«èª¬æ˜

## åˆ¤æ–­åŸºæº–
- **BUY**: ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãŒæ˜ç¢ºã§ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã«é©ã—ãŸçŠ¶æ³
- **SELL**: ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ãŒæ˜ç¢ºã§ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã«é©ã—ãŸçŠ¶æ³
- **HOLD**: ãƒˆãƒ¬ãƒ³ãƒ‰ãŒä¸æ˜ç¢ºã€ã¾ãŸã¯ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã«ä¸é©åˆ‡ãªçŠ¶æ³

## é‡è¦äº‹é …
- è¤‡æ•°ã®æ™‚é–“è¶³ãŒåŒã˜æ–¹å‘ã‚’ç¤ºã—ã¦ã„ã‚‹å ´åˆã€ä¿¡é ¼åº¦ã‚’é«˜ãã™ã‚‹
- ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ãŒçŸ›ç›¾ã—ã¦ã„ã‚‹å ´åˆã¯ã€æ…é‡ã«HOLDã‚’é¸æŠ
- ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒé«˜ã™ãã‚‹/ä½ã™ãã‚‹å ´åˆã¯è€ƒæ…®ã«å…¥ã‚Œã‚‹

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ï¼‰:

```json
{{
  "action": "BUY/SELL/HOLD",
  "confidence": 0-100ã®æ•°å€¤,
  "reasoning": "åˆ¤æ–­ç†ç”±ã‚’è©³ã—ãèª¬æ˜ï¼ˆå„æ™‚é–“è¶³ã®çŠ¶æ³ã€ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã®çŠ¶æ…‹ã€ç·åˆåˆ¤æ–­ã®æ ¹æ‹ ï¼‰",
  "entry_price": ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¨å¥¨ä¾¡æ ¼ï¼ˆactionãŒHOLDä»¥å¤–ã®å ´åˆï¼‰,
  "stop_loss": ã‚¹ãƒˆãƒƒãƒ—ãƒ­ã‚¹æ¨å¥¨ä¾¡æ ¼ï¼ˆactionãŒHOLDä»¥å¤–ã®å ´åˆï¼‰,
  "take_profit": ãƒ†ã‚¤ã‚¯ãƒ—ãƒ­ãƒ•ã‚£ãƒƒãƒˆæ¨å¥¨ä¾¡æ ¼ï¼ˆactionãŒHOLDä»¥å¤–ã®å ´åˆï¼‰
}}
```

æ³¨æ„: å¿…ãšJSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯"reasoning"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å«ã‚ã¦ãã ã•ã„ã€‚
"""
        return prompt

    def _select_model(self, model: str):
        """
        ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹

        Args:
            model: ãƒ¢ãƒ‡ãƒ«åï¼ˆå®Œå…¨ãªãƒ¢ãƒ‡ãƒ«å ä¾‹: gemini-2.5-flashï¼‰
                ã¾ãŸã¯çŸ­ç¸®åï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰:
                - 'daily_analysis': MODEL_DAILY_ANALYSISã®å€¤ã‚’ä½¿ç”¨
                - 'periodic_update': MODEL_PERIODIC_UPDATEã®å€¤ã‚’ä½¿ç”¨
                - 'position_monitor': MODEL_POSITION_MONITORã®å€¤ã‚’ä½¿ç”¨
                - 'emergency_evaluation': MODEL_EMERGENCY_EVALUATIONã®å€¤ã‚’ä½¿ç”¨

        Returns:
            Tuple[GenerativeModel, str]: (é¸æŠã•ã‚ŒãŸGenerativeModelã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ, å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å)
        """
        from src.utils.config import get_config
        config = get_config()

        # çŸ­ç¸®åã‹ã‚‰.envè¨­å®šã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        # æ³¨: .envãƒ•ã‚¡ã‚¤ãƒ«ã§é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«åã‚’è¨­å®šã—ã¦ãã ã•ã„
        phase_to_config_mapping = {
            'daily_analysis': config.model_daily_analysis,
            'periodic_update': config.model_periodic_update,
            'position_monitor': config.model_position_monitor,
            'emergency_evaluation': config.model_emergency_evaluation,
            # å¤ã„çŸ­ç¸®åï¼ˆéæ¨å¥¨ã€å¾Œæ–¹äº’æ›æ€§ã®ã¿ï¼‰
            'pro': config.model_daily_analysis,
            'flash': config.model_periodic_update,
            'flash-8b': config.model_position_monitor,
            'flash-lite': config.model_position_monitor,
        }

        # çŸ­ç¸®åã®å ´åˆã¯.envã‹ã‚‰è¨­å®šã‚’å–å¾—
        if model in phase_to_config_mapping:
            model_name = phase_to_config_mapping[model]
            if not model_name:
                raise ValueError(
                    f"Model for phase '{model}' is not configured in .env file. "
                    f"Please set the appropriate MODEL_* environment variable."
                )
            self.logger.debug(f"Model phase '{model}' mapped to '{model_name}' from .env configuration")
        else:
            # ã™ã§ã«å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: gemini-2.5-flash, claude-sonnet-4-5ãªã©ï¼‰
            model_name = model

        # ãƒ¢ãƒ‡ãƒ«åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ - GeminiClient ã¯Geminiãƒ¢ãƒ‡ãƒ«ã®ã¿å¯¾å¿œ
        if not model_name.startswith('gemini-'):
            # Geminiä»¥å¤–ã®ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
            provider_hint = "Unknown"
            if model_name.startswith('claude-'):
                provider_hint = "Anthropic Claude"
            elif model_name.startswith(('gpt-', 'o1-', 'chatgpt-')):
                provider_hint = "OpenAI"

            raise ValueError(
                f"GeminiClient cannot use non-Gemini model: '{model_name}' ({provider_hint})\n"
                f"Please configure a Gemini model (gemini-*) in your .env file.\n"
                f"Example Gemini models:\n"
                f"  - gemini-2.0-flash-exp\n"
                f"  - gemini-1.5-flash\n"
                f"  - gemini-1.5-flash-8b\n"
                f"  - gemini-1.5-pro\n"
                f"\n"
                f"If you want to use {provider_hint} models, the system needs to be updated\n"
                f"to use the multi-provider architecture with appropriate client selection."
            )

        # GenerativeModelã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã¦ã€ãƒ¢ãƒ‡ãƒ«åã‚‚è¿”ã™
        return genai.GenerativeModel(model_name), model_name

    def _parse_response(self, response_text: str) -> Dict:
        """
        AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹

        AIã®å¿œç­”ã‹ã‚‰JSONéƒ¨åˆ†ã‚’æŠ½å‡ºã—ã€è¾æ›¸ã«å¤‰æ›ã—ã¾ã™ã€‚
        ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆHOLDï¼‰ã‚’è¿”ã—ã¾ã™ã€‚

        Args:
            response_text: AIã‹ã‚‰ã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸåˆ¤æ–­çµæœã®è¾æ›¸
        """
        try:
            # JSONãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ```json ... ```ï¼‰ã‚’æŠ½å‡º
            json_match = re.search(
                r'```json\s*(\{.*?\})\s*```',
                response_text,
                re.DOTALL
            )

            if json_match:
                json_text = json_match.group(1)
            else:
                # JSONãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆã€{ } ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’æ¢ã™
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(0)
                else:
                    # JSONå½¢å¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
                    raise ValueError("No JSON format found in response")

            # JSONã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(json_text)

            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼
            if 'action' not in result:
                raise ValueError("'action' field is missing in response")

            if result['action'] not in ['BUY', 'SELL', 'HOLD']:
                raise ValueError(f"Invalid action: {result['action']}")

            # confidenceã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            if 'confidence' not in result:
                result['confidence'] = 50

            # reasoningã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            if 'reasoning' not in result:
                result['reasoning'] = 'No reasoning provided'

            return result

        except (json.JSONDecodeError, ValueError) as e:
            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            self.logger.error(f"Failed to parse AI response: {e}")
            self.logger.debug(f"Response text: {response_text}")

            return {
                'action': 'HOLD',
                'confidence': 0,
                'reasoning': f'Failed to parse AI response: {str(e)}'
            }

    def test_connection(self, verbose: bool = False, model: Optional[str] = None) -> bool:
        """
        Gemini APIã¸ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ

        ç°¡å˜ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã—ã¦APIãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚

        Args:
            verbose: è©³ç´°ãªãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
            model: ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ï¼‰

        Returns:
            True: æ¥ç¶šæˆåŠŸ, False: æ¥ç¶šå¤±æ•—
        """
        try:
            if verbose:
                print("ğŸ”Œ Gemini APIæ¥ç¶šãƒ†ã‚¹ãƒˆä¸­...", end='', flush=True)

            # ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆæœ€ã‚‚è»½é‡ã§é«˜é€Ÿï¼‰ã‚’ä½¿ç”¨
            test_model = model if model else 'gemini-2.0-flash-lite'

            test_prompt = "Hello, this is a connection test. Please respond with 'OK'."
            # generate_responseã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²
            response = self.generate_response(
                prompt=test_prompt,
                model=test_model,
                max_tokens=10,
                phase="Connection Test"  # ãƒ¬ãƒãƒ¼ãƒˆã§è­˜åˆ¥ã§ãã‚‹ã‚ˆã†ã«phaseã‚’è¨­å®š
            )

            if response:
                if verbose:
                    print(" âœ“ æ¥ç¶šæˆåŠŸ")
                return True
            else:
                if verbose:
                    print(" âŒ å¤±æ•—ï¼ˆç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼‰")
                self.logger.error("Gemini API connection test failed: empty response")
                return False

        except Exception as e:
            if verbose:
                print(f" âŒ å¤±æ•—")
                print(f"   ã‚¨ãƒ©ãƒ¼: {e}")
            self.logger.error(f"Gemini API connection test failed: {e}")
            return False

    def get_provider_name(self) -> str:
        """
        ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã‚’å–å¾—

        Returns:
            str: "gemini"
        """
        return "gemini"


# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = ['GeminiClient']
