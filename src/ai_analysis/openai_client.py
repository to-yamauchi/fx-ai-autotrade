"""
========================================
OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
========================================

ãƒ•ã‚¡ã‚¤ãƒ«å: openai_client.py
ãƒ‘ã‚¹: src/ai_analysis/openai_client.py

ã€æ¦‚è¦ã€‘
OpenAI ChatGPT APIã‚’ä½¿ç”¨ã—ã¦LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã™ã€‚
BaseLLMClientã‚’ç¶™æ‰¿ã—ã€çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

ã€å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‘
- GPT-5ã‚·ãƒªãƒ¼ã‚º: gpt-5-nano, gpt-5-mini (Responses API)
- GPT-4ã‚·ãƒªãƒ¼ã‚º: gpt-4o, gpt-4o-mini, gpt-4-turbo (Chat Completions API)
- GPT-3.5ã‚·ãƒªãƒ¼ã‚º: gpt-3.5-turbo (Chat Completions API)
- o1ã‚·ãƒªãƒ¼ã‚º: o1-preview, o1-mini (Chat Completions API)

æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§: https://platform.openai.com/docs/models

ã€APIç¨®åˆ¥ã€‘
- GPT-5: Responses API (client.responses.create)
- GPT-4/3.5/o1: Chat Completions API (client.chat.completions.create)

ã€ä½¿ç”¨ä¾‹ã€‘
```python
from src.ai_analysis.openai_client import OpenAIClient

client = OpenAIClient(api_key="your_api_key")
response = client.generate_response(
    prompt="Analyze this market data...",
    model="gpt-4o",
    temperature=0.3,
    max_tokens=2000
)
```

ã€ä½œæˆæ—¥ã€‘2025-10-23
"""

from typing import Optional, Dict
import logging
import time
import json
import re
from openai import OpenAI, InternalServerError, RateLimitError
from src.ai_analysis.base_llm_client import BaseLLMClient


class OpenAIClient(BaseLLMClient):
    """
    OpenAI ChatGPT APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

    OpenAI APIã‚’ä½¿ç”¨ã—ã¦LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """

    def __init__(self, api_key: str):
        """
        OpenAIClientã®åˆæœŸåŒ–

        Args:
            api_key: OpenAI APIã‚­ãƒ¼
        """
        super().__init__(api_key)
        self.client = OpenAI(api_key=api_key)
        self.logger.info("OpenAI client initialized")

    def _select_model(self, model: str) -> str:
        """
        ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹

        Args:
            model: ãƒ¢ãƒ‡ãƒ«åï¼ˆå®Œå…¨ãªãƒ¢ãƒ‡ãƒ«å ä¾‹: gpt-4oï¼‰
                ã¾ãŸã¯çŸ­ç¸®åï¼ˆPhaseåï¼‰:
                - 'daily_analysis': MODEL_DAILY_ANALYSISã®å€¤ã‚’ä½¿ç”¨
                - 'periodic_update': MODEL_PERIODIC_UPDATEã®å€¤ã‚’ä½¿ç”¨
                - 'position_monitor': MODEL_POSITION_MONITORã®å€¤ã‚’ä½¿ç”¨
                - 'emergency_evaluation': MODEL_EMERGENCY_EVALUATIONã®å€¤ã‚’ä½¿ç”¨

        Returns:
            str: å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å

        Raises:
            ValueError: ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒä¸æ­£ãªå ´åˆ
        """
        from src.utils.config import get_config
        config = get_config()

        # Phaseåã‹ã‚‰.envè¨­å®šã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        phase_to_config_mapping = {
            'daily_analysis': config.model_daily_analysis,
            'periodic_update': config.model_periodic_update,
            'position_monitor': config.model_position_monitor,
            'emergency_evaluation': config.model_emergency_evaluation,
        }

        # Phaseåã®å ´åˆã¯.envã‹ã‚‰è¨­å®šã‚’å–å¾—
        if model in phase_to_config_mapping:
            model_name = phase_to_config_mapping[model]
            if not model_name:
                raise ValueError(
                    f"Model for phase '{model}' is not configured in .env file. "
                    f"Please set the appropriate MODEL_* environment variable."
                )
            self.logger.debug(f"Phase '{model}' mapped to model '{model_name}'")
        else:
            # ã™ã§ã«å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«å
            model_name = model

        # ãƒ¢ãƒ‡ãƒ«åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ - OpenAIClient ã¯OpenAIãƒ¢ãƒ‡ãƒ«ã®ã¿å¯¾å¿œ
        if not model_name.startswith(('gpt-', 'o1-', 'o3-', 'chatgpt-')):
            # OpenAIä»¥å¤–ã®ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
            provider_hint = "Unknown"
            if model_name.startswith('gemini-'):
                provider_hint = "Google Gemini"
            elif model_name.startswith('claude-'):
                provider_hint = "Anthropic Claude"

            raise ValueError(
                f"OpenAIClient cannot use non-OpenAI model: '{model_name}' ({provider_hint})\n"
                f"Please configure an OpenAI model (gpt-*, o1-*, o3-*, chatgpt-*) in your .env file.\n"
                f"Example OpenAI models:\n"
                f"  - gpt-5-nano, gpt-5-mini (Responses API)\n"
                f"  - gpt-4o, gpt-4o-mini (Chat Completions API)\n"
                f"  - o1-preview, o1-mini (Chat Completions API)\n"
                f"\n"
                f"If you want to use {provider_hint} models, configure them in MODEL_* variables\n"
                f"and the system will automatically select the appropriate client."
            )

        return model_name

    def generate_response(
        self,
        prompt: str,
        model: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        OpenAI APIã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ

        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
            model: ãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: gpt-4o, gpt-4o-miniï¼‰
                  ã¾ãŸã¯Phaseåï¼ˆä¾‹: daily_analysis, periodic_updateï¼‰
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0.0-2.0ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtop_p, frequency_penalty, etc.ï¼‰

        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ

        Raises:
            Exception: APIå‘¼ã³å‡ºã—ãŒå¤±æ•—ã—ãŸå ´åˆ
        """
        try:
            # Phaseåã‚’å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›
            actual_model = self._select_model(model)

            # phaseãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è¨˜éŒ²ç”¨ï¼‰
            phase = kwargs.pop('phase', 'Unknown')

            # GPT-5ã¯ Responses APIã€ãã‚Œä»¥å¤–ã¯ Chat Completions API
            is_gpt5 = actual_model.startswith('gpt-5')

            # ãƒ­ã‚°å‡ºåŠ›
            api_type = "Responses API" if is_gpt5 else "Chat Completions API"
            self.logger.debug(
                f"OpenAI {api_type} request: model={actual_model}, "
                f"temperature={temperature if not is_gpt5 else 'N/A'}, "
                f"max_tokens={max_tokens}"
            )

            # APIå‘¼ã³å‡ºã—ï¼ˆãƒªãƒˆãƒ©ã‚¤å‡¦ç†ä»˜ãï¼‰
            max_retries = 3
            retry_delay = 2  # åˆå›å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

            for attempt in range(max_retries):
                try:
                    if is_gpt5:
                        # GPT-5: Responses API
                        response = self._call_responses_api(
                            model=actual_model,
                            prompt=prompt,
                            max_tokens=max_tokens,
                            **kwargs
                        )
                    else:
                        # GPT-4/3.5/o1: Chat Completions API
                        response = self._call_chat_completions_api(
                            model=actual_model,
                            prompt=prompt,
                            temperature=temperature,
                            max_tokens=max_tokens,
                            **kwargs
                        )
                    break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹

                except (InternalServerError, RateLimitError) as e:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•: 2ç§’ã€4ç§’ã€8ç§’
                        self.logger.warning(
                            f"OpenAI API error (attempt {attempt + 1}/{max_retries}): {e}. "
                            f"Retrying in {wait_time} seconds..."
                        )
                        time.sleep(wait_time)
                    else:
                        # æœ€å¾Œã®ãƒªãƒˆãƒ©ã‚¤ã‚‚å¤±æ•—
                        self.logger.error(f"OpenAI API failed after {max_retries} attempts: {e}")
                        raise

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if is_gpt5:
                text = self._extract_text_from_responses_api(response)
            else:
                text = self._extract_text_from_chat_completions_api(response, max_tokens)

            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²
            if hasattr(response, 'usage'):
                from src.ai_analysis.token_usage_tracker import get_token_tracker
                tracker = get_token_tracker()
                tracker.record_usage(
                    phase=phase,
                    provider='openai',
                    model=actual_model,
                    input_tokens=response.usage.prompt_tokens if hasattr(response.usage, 'prompt_tokens') else 0,
                    output_tokens=response.usage.completion_tokens if hasattr(response.usage, 'completion_tokens') else 0
                )

            return text

        except Exception as e:
            self.logger.error(f"OpenAI API error: {e}")
            raise

    def _call_chat_completions_api(
        self,
        model: str,
        prompt: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        Chat Completions APIã‚’å‘¼ã³å‡ºã™ï¼ˆGPT-4/3.5/o1ç”¨ï¼‰

        Args:
            model: ãƒ¢ãƒ‡ãƒ«å
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            ChatCompletion: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        params = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
        }

        # o1ã‚·ãƒªãƒ¼ã‚ºã‹ã©ã†ã‹ã‚’åˆ¤å®š
        is_o1_model = model.startswith(('o1-', 'o3-'))

        # temperatureã®è¨­å®šï¼ˆo1ã‚·ãƒªãƒ¼ã‚ºã¯éå¯¾å¿œï¼‰
        if temperature is not None and not is_o1_model:
            params["temperature"] = temperature

        # max_tokensã®è¨­å®šï¼ˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ä½¿ã„åˆ†ã‘ï¼‰
        if max_tokens is not None:
            if is_o1_model:
                # o1ã‚·ãƒªãƒ¼ã‚ºã¯ max_completion_tokens ã‚’ä½¿ç”¨
                params["max_completion_tokens"] = max_tokens
            else:
                # GPT-4ã€GPT-3.5ãªã©ã¯ max_tokens ã‚’ä½¿ç”¨
                params["max_tokens"] = max_tokens

        # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        params.update(kwargs)

        return self.client.chat.completions.create(**params)

    def _call_responses_api(
        self,
        model: str,
        prompt: str,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        Responses APIã‚’å‘¼ã³å‡ºã™ï¼ˆGPT-5ç”¨ï¼‰

        Args:
            model: ãƒ¢ãƒ‡ãƒ«å
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

        Returns:
            Response: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        params = {
            "model": model,
            "input": [
                {"type": "message", "role": "user", "content": prompt}
            ],
            "text": {
                "format": {"type": "text"},
                "verbosity": "medium"
            },
            "reasoning": {
                "effort": "medium",
                "summary": "auto"
            }
        }

        # max_tokensãŒã‚ã‚‹å ´åˆã¯ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦è¨­å®š
        if max_tokens is not None:
            params["max_output_tokens"] = max_tokens

        # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        params.update(kwargs)

        return self.client.responses.create(**params)

    def _extract_text_from_chat_completions_api(self, response, max_tokens: Optional[int]) -> str:
        """
        Chat Completions APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º

        Args:
            response: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
            max_tokens: max_tokensè¨­å®šå€¤

        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ

        Raises:
            ValueError: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã¾ãŸã¯ç•°å¸¸ãªå ´åˆ
        """
        if not response.choices:
            raise ValueError("OpenAI API returned no choices")

        text = response.choices[0].message.content

        # finish_reasonã‚’ãƒã‚§ãƒƒã‚¯
        finish_reason = response.choices[0].finish_reason
        if finish_reason == "length":
            self.logger.warning(
                f"Response was truncated due to max_tokens limit. "
                f"Current max_tokens: {max_tokens}. "
                f"Consider increasing max_tokens in .env"
            )
        elif finish_reason == "content_filter":
            raise ValueError(
                "Response was filtered by OpenAI content policy. "
                "Please modify your prompt."
            )

        self.logger.debug(
            f"OpenAI API response received: "
            f"finish_reason={finish_reason}, "
            f"length={len(text)} chars"
        )

        return text

    def _extract_text_from_responses_api(self, response) -> str:
        """
        Responses APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º

        Args:
            response: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ (openai.types.responses.Response)

        Returns:
            str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ

        Raises:
            ValueError: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã¾ãŸã¯ç•°å¸¸ãªå ´åˆ
        """
        # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°æƒ…å ±ã‚’ç¢ºèª
        status = response.status if hasattr(response, 'status') else 'N/A'
        self.logger.debug(f"Response status: {status}")
        self.logger.debug(f"Response output length: {len(response.output) if hasattr(response, 'output') and response.output else 0}")

        # incompleteã®å ´åˆã€è©³ç´°æƒ…å ±ã‚’ç¢ºèª
        if status == 'incomplete' and hasattr(response, 'incomplete_details'):
            details = response.incomplete_details
            reason = details.reason if hasattr(details, 'reason') else 'unknown'
            self.logger.warning(f"Response is incomplete. Reason: {reason}")
            if reason == 'max_output_tokens':
                self.logger.warning("Response was truncated due to max_output_tokens limit. Consider increasing max_tokens.")

        # GPT-5ã¯éåŒæœŸã§å®Ÿè¡Œã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å®Œäº†ã‚’å¾…ã¤
        if status in ['in_progress', 'queued']:
            self.logger.info(f"Response status is '{status}', waiting for completion...")
            max_wait_attempts = 30  # æœ€å¤§30å›ï¼ˆ30ç§’ï¼‰å¾…æ©Ÿ
            wait_interval = 1  # 1ç§’é–“éš”

            for attempt in range(max_wait_attempts):
                time.sleep(wait_interval)
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹IDã‚’ä½¿ã£ã¦æœ€æ–°ã®çŠ¶æ…‹ã‚’å–å¾—
                response = self.client.responses.retrieve(response.id)
                status = response.status if hasattr(response, 'status') else 'N/A'
                self.logger.debug(f"Response status (attempt {attempt + 1}): {status}")

                if status == 'completed':
                    self.logger.info(f"Response completed after {attempt + 1} seconds")
                    break
                elif status in ['failed', 'cancelled', 'incomplete']:
                    self.logger.error(f"Response ended with status: {status}")
                    break

            if status != 'completed':
                raise ValueError(f"Response did not complete. Final status: {status}")

        # outputé…åˆ—ã®å†…å®¹ã‚’ç¢ºèª
        if hasattr(response, 'output') and response.output:
            for i, output_item in enumerate(response.output):
                self.logger.debug(f"Output[{i}] type: {output_item.type if hasattr(output_item, 'type') else 'N/A'}")
                if hasattr(output_item, 'content') and output_item.content:
                    self.logger.debug(f"Output[{i}] content length: {len(output_item.content)}")
                    for j, content_item in enumerate(output_item.content):
                        content_type = content_item.type if hasattr(content_item, 'type') else 'N/A'
                        self.logger.debug(f"Output[{i}] content[{j}] type: {content_type}")
                        if content_type == 'text' and hasattr(content_item, 'text'):
                            self.logger.debug(f"Output[{i}] content[{j}] text length: {len(content_item.text)}")

        # OpenAI SDKã®Responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¯output_textãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒã‚ã‚‹
        # ã“ã‚ŒãŒã™ã¹ã¦ã®output_textã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é›†ç´„ã—ãŸã‚‚ã®
        # ãŸã ã—ã€response.outputãŒNoneã®å ´åˆã€output_textã‚¢ã‚¯ã‚»ã‚¹æ™‚ã«ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
        text = ""

        if not hasattr(response, 'output_text'):
            self.logger.error(f"Response has no 'output_text' property. Type: {type(response)}")
            raise ValueError("OpenAI Responses API returned unexpected response type")

        # output_textã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å‰ã«ã€outputãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèª
        if hasattr(response, 'output') and response.output is not None:
            try:
                text = response.output_text
            except (TypeError, AttributeError) as e:
                self.logger.warning(f"Failed to access output_text property: {e}")
                text = ""
        else:
            self.logger.warning(f"Response.output is None, cannot access output_text property")

        # output_textãŒç©ºã®å ´åˆã€ä»£æ›¿æ‰‹æ®µã‚’è©¦ã™
        if not text:
            self.logger.warning("OpenAI Responses API returned empty output_text")

            # ä»£æ›¿: outputé…åˆ—ã‹ã‚‰ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if hasattr(response, 'output') and response.output:
                texts = []
                for output_item in response.output:
                    if hasattr(output_item, 'content') and output_item.content:
                        for content_item in output_item.content:
                            # 'output_text'ã‚¿ã‚¤ãƒ—ã¾ãŸã¯'text'ã‚¿ã‚¤ãƒ—ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™
                            content_type = content_item.type if hasattr(content_item, 'type') else None
                            if content_type in ['output_text', 'text']:
                                if hasattr(content_item, 'text'):
                                    texts.append(content_item.text)
                                    self.logger.debug(f"Found '{content_type}' type content: {len(content_item.text)} chars")
                            else:
                                self.logger.debug(f"Skipping content type: {content_type}")

                if texts:
                    text = "".join(texts)
                    self.logger.info(f"Extracted text from output.content: {len(text)} chars")
                else:
                    self.logger.warning("No text content found in output.content array")
            else:
                self.logger.error(f"Response output is None or empty. Response ID: {response.id if hasattr(response, 'id') else 'N/A'}")

        if not text:
            # ãã‚Œã§ã‚‚ç©ºã®å ´åˆã¯è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
            model_name = response.model if hasattr(response, 'model') else 'Unknown'
            error_msg = f"No text content found in response from model '{model_name}'"

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°æƒ…å ±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            if hasattr(response, 'id'):
                error_msg += f"\nResponse ID: {response.id}"
            if hasattr(response, 'status'):
                error_msg += f"\nStatus: {response.status}"

            # outputã®çŠ¶æ…‹ã‚’ç¢ºèª
            if hasattr(response, 'output'):
                if response.output is None:
                    error_msg += "\noutput: None (no output generated)"
                elif len(response.output) == 0:
                    error_msg += "\noutput: [] (empty array)"
                else:
                    error_msg += f"\noutput: {len(response.output)} items, but no text content found"

            self.logger.error(error_msg)

            # gpt-5-nanoã®å ´åˆã¯ç‰¹åˆ¥ãªãƒ’ãƒ³ãƒˆã‚’è¿½åŠ 
            if model_name and 'gpt-5-nano' in model_name:
                self.logger.warning(
                    "gpt-5-nano may have different behavior or requirements. "
                    "Consider using gpt-5-mini or checking model availability."
                )

            raise ValueError(f"OpenAI Responses API returned no text content for model '{model_name}'")

        self.logger.debug(
            f"OpenAI Responses API response received: "
            f"length={len(text)} chars"
        )

        return text

    def test_connection(self, verbose: bool = False, model: Optional[str] = None) -> bool:
        """
        OpenAI APIã¸ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ

        Args:
            verbose: è©³ç´°ãªãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
            model: ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ï¼‰

        Returns:
            bool: True=æ¥ç¶šæˆåŠŸ, False=æ¥ç¶šå¤±æ•—
        """
        try:
            if verbose:
                print("ğŸ”Œ OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆä¸­...", end='', flush=True)

            # ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆæœ€ã‚‚å®‰ä¾¡ï¼‰ã‚’ä½¿ç”¨
            test_model = model if model else "gpt-3.5-turbo"

            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡
            # max_tokensã¯æŒ‡å®šã—ãªã„ï¼ˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œã«ä»»ã›ã‚‹ï¼‰
            # ã“ã‚Œã«ã‚ˆã‚Šã€GPT-5ã®reasoningãŒé€”ä¸­ã§åˆ‡ã‚Œã‚‹å•é¡Œã‚’å›é¿
            test_prompt = "Say OK"
            response = self.generate_response(
                prompt=test_prompt,
                model=test_model,
                max_tokens=None,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œã«ä»»ã›ã‚‹
                phase="Connection Test"  # ãƒ¬ãƒãƒ¼ãƒˆã§è­˜åˆ¥ã§ãã‚‹ã‚ˆã†ã«phaseã‚’è¨­å®š
            )

            if response:
                if verbose:
                    print(" âœ“ æ¥ç¶šæˆåŠŸ")
                self.logger.info("OpenAI API connection test: SUCCESS")
                return True
            else:
                if verbose:
                    print(" âœ— æ¥ç¶šå¤±æ•—")
                self.logger.error("OpenAI API connection test: FAILED (empty response)")
                return False

        except Exception as e:
            if verbose:
                print(f" âœ— æ¥ç¶šå¤±æ•—: {e}")
            self.logger.error(f"OpenAI API connection test: FAILED - {e}")
            return False

    def get_provider_name(self) -> str:
        """
        ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã‚’å–å¾—

        Returns:
            str: "openai"
        """
        return "openai"

    def analyze_market(self,
                      market_data: Dict,
                      model: str = 'gpt-4o') -> Dict:
        """
        ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‰åˆ¤æ–­ã‚’è¡Œã†

        Args:
            market_data: æ¨™æº–åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆDataStandardizerã®å‡ºåŠ›ï¼‰
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« (ä¾‹: 'gpt-4o', 'periodic_update')

        Returns:
            AIåˆ¤æ–­çµæœã®è¾æ›¸
            {
                'action': 'BUY' | 'SELL' | 'HOLD',
                'confidence': 0-100ã®æ•°å€¤,
                'reasoning': 'åˆ¤æ–­ç†ç”±ã®èª¬æ˜',
                'entry_price': ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¨å¥¨ä¾¡æ ¼ (optional),
                'stop_loss': SLæ¨å¥¨ä¾¡æ ¼ (optional),
                'take_profit': TPæ¨å¥¨ä¾¡æ ¼ (optional)
            }

        Raises:
            Exception: APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼æ™‚ï¼ˆã‚¨ãƒ©ãƒ¼ã¯ãƒ­ã‚°ã«è¨˜éŒ²ã—ã€HOLDã‚’è¿”ã™ï¼‰
        """
        try:
            # åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
            prompt = self._build_analysis_prompt(market_data)

            # generate_responseã‚’ä½¿ç”¨ã—ã¦AIåˆ†æã‚’å®Ÿè¡Œ
            response = self.generate_response(
                prompt=prompt,
                model=model,
                phase='Market Analysis'
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‘ãƒ¼ã‚¹
            result = self._parse_response(response)

            return result

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯HOLDã‚’è¿”ã™
            self.logger.error(f"AI analysis error: {e}")
            return {
                'action': 'HOLD',
                'confidence': 0,
                'reasoning': f'Error occurred during AI analysis: {str(e)}'
            }

    def _build_analysis_prompt(self, market_data: Dict) -> str:
        """
        åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹

        Args:
            market_data: æ¨™æº–åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿

        Returns:
            åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """
        # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
        market_data_json = json.dumps(market_data, indent=2, ensure_ascii=False)

        prompt = f"""ã‚ãªãŸã¯ãƒ—ãƒ­ã®FXã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã§ã™ã€‚10-30pipsã®å°ã•ãªåˆ©ç›Šã‚’ç©æ¥µçš„ã«ç‹™ã„ã¾ã™ã€‚ä»¥ä¸‹ã®ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‰åˆ¤æ–­ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

## ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿
{market_data_json}

## ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ã‚¿ã‚¤ãƒ«
- **ã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°é‡è¦–**: 10-30pipsã®å°ã•ãªå€¤å¹…ã§ã‚‚ç©æ¥µçš„ã«ã‚¨ãƒ³ãƒˆãƒªãƒ¼
- **M15ï¼ˆ15åˆ†è¶³ï¼‰ã‚’æœ€é‡è¦–**: ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¯M15ã‚’ä¸­å¿ƒã«åˆ¤æ–­
- **ç©æ¥µçš„ãªå§¿å‹¢**: ãƒ¬ãƒ³ã‚¸ç›¸å ´ã§ã‚‚åç™ºãƒ»æŠ¼ã—ç›®ã‚’ç‹™ã†
- **è¿…é€Ÿãªåˆ¤æ–­**: æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰ãŒãªãã¦ã‚‚ã€çŸ­æœŸçš„ãªæ–¹å‘æ€§ãŒã‚ã‚Œã°ã‚¨ãƒ³ãƒˆãƒªãƒ¼

## åˆ†ææŒ‡ç¤º
1. **M15ï¼ˆ15åˆ†è¶³ï¼‰ã®è©³ç´°åˆ†æï¼ˆæœ€é‡è¦ï¼‰**
   - ç›´è¿‘ã®ä¾¡æ ¼ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä¸Šæ˜‡/ä¸‹é™ã®å‹¢ã„ï¼‰
   - EMAã¨ã®ä½ç½®é–¢ä¿‚ï¼ˆã‚¯ãƒ­ã‚¹ã‚„ã‚¿ãƒƒãƒï¼‰
   - RSIã®çŠ¶æ…‹ï¼ˆ30ä»¥ä¸‹ã§è²·ã„ã€70ä»¥ä¸Šã§å£²ã‚Šã‚·ã‚°ãƒŠãƒ«ï¼‰
   - ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ã®ä½ç½®ï¼ˆãƒãƒ³ãƒ‰ã‚¿ãƒƒãƒã¯åè»¢ã‚·ã‚°ãƒŠãƒ«ï¼‰

2. **çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã®ç¢ºèªï¼ˆH1ï¼‰**
   - M15ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ãŒçŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã«æ²¿ã£ã¦ã„ã‚‹ã‹ç¢ºèª
   - é€†å¼µã‚Šã®å ´åˆã¯ç¢ºä¿¡åº¦ã‚’ä¸‹ã’ã‚‹

3. **ä¸­é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã®ç¢ºèªï¼ˆH4ã€D1ï¼‰**
   - é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã¨åŒã˜æ–¹å‘ãªã‚‰ç¢ºä¿¡åº¦ã‚’ä¸Šã’ã‚‹
   - é€†æ–¹å‘ã§ã‚‚M15ãŒæ˜ç¢ºãªã‚‰ã‚¨ãƒ³ãƒˆãƒªãƒ¼å¯ï¼ˆç¢ºä¿¡åº¦ã¯ä¸‹ã’ã‚‹ï¼‰

4. **ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã®ç·åˆåˆ¤æ–­**
   - RSI: 30ä»¥ä¸‹=è²·ã„ãƒãƒ£ãƒ³ã‚¹ã€70ä»¥ä¸Š=å£²ã‚Šãƒãƒ£ãƒ³ã‚¹ã€40-60=ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚©ãƒ­ãƒ¼
   - MACD: ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®æ–¹å‘è»¢æ›ã‚’ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚·ã‚°ãƒŠãƒ«ã¨ã—ã¦é‡è¦–
   - ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼: ãƒãƒ³ãƒ‰ã®ä¸Šé™/ä¸‹é™ã‚¿ãƒƒãƒã¯åè»¢ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒãƒ£ãƒ³ã‚¹
   - EMA: ä¾¡æ ¼ãŒEMAã‚’ä¸ŠæŠœã‘/ä¸‹æŠœã‘ã—ãŸç›´å¾Œã¯ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒãƒ£ãƒ³ã‚¹

## åˆ¤æ–­åŸºæº–ï¼ˆã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°é‡è¦–ï¼‰
- **BUYæ¡ä»¶**:
  - M15ã§ä¸Šæ˜‡ã®å‹¢ã„ãŒã‚ã‚‹
  - RSI < 70ï¼ˆè²·ã‚ã‚Œã™ãã§ãªã‘ã‚Œã°OKï¼‰
  - ä¾¡æ ¼ãŒEMAä¸Šã«ã‚ã‚‹ã€ã¾ãŸã¯EMAã‚’ä¸ŠæŠœã‘ãŸç›´å¾Œ
  - ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ä¸‹é™ä»˜è¿‘ã‹ã‚‰ã®åç™º
  - MACDãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãŒãƒ—ãƒ©ã‚¹ã«è»¢ã˜ãŸ

- **SELLæ¡ä»¶**:
  - M15ã§ä¸‹é™ã®å‹¢ã„ãŒã‚ã‚‹
  - RSI > 30ï¼ˆå£²ã‚‰ã‚Œã™ãã§ãªã‘ã‚Œã°OKï¼‰
  - ä¾¡æ ¼ãŒEMAä¸‹ã«ã‚ã‚‹ã€ã¾ãŸã¯EMAã‚’ä¸‹æŠœã‘ãŸç›´å¾Œ
  - ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ä¸Šé™ä»˜è¿‘ã‹ã‚‰ã®åè½
  - MACDãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ãŒãƒã‚¤ãƒŠã‚¹ã«è»¢ã˜ãŸ

- **HOLDæ¡ä»¶ï¼ˆæœ€å°é™ã«ï¼‰**:
  - ã™ã¹ã¦ã®æŒ‡æ¨™ãŒå®Œå…¨ã«ä¸­ç«‹ï¼ˆRSI 45-55ã€MACD 0ä»˜è¿‘ã€EMAãƒ•ãƒ©ãƒƒãƒˆï¼‰
  - é‡è¦ãªçµŒæ¸ˆæŒ‡æ¨™ç™ºè¡¨ã®ç›´å‰

## é‡è¦äº‹é …
- **HOLDã¯æœ€å¾Œã®é¸æŠè‚¢**: å°‘ã—ã§ã‚‚ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒãƒ£ãƒ³ã‚¹ãŒã‚ã‚Œã°BUY/SELLã‚’é¸æŠ
- **å°ã•ãªåˆ©ç›Šã‚’ç‹™ã†**: 10pipsç¨‹åº¦ã®å°ã•ãªå‹•ãã§ã‚‚ç©æ¥µçš„ã«ã‚¨ãƒ³ãƒˆãƒªãƒ¼
- **ç¢ºä¿¡åº¦ã¯50ä»¥ä¸Šã‚’ç›®æ¨™**: å®Œç’§ãªçŠ¶æ³ã‚’å¾…ãŸãšã€60-70%ã®ç¢ºä¿¡åº¦ã§ã‚‚ã‚¨ãƒ³ãƒˆãƒªãƒ¼
- **ã‚¹ãƒˆãƒƒãƒ—ã¯ç‹­ã**: 10-15pipsç¨‹åº¦ã®ã‚¿ã‚¤ãƒˆãªã‚¹ãƒˆãƒƒãƒ—ã‚’æ¨å¥¨
- **ãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰**: æœ€ä½1:1ã€ç†æƒ³ã¯1:1.5ä»¥ä¸Š

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ï¼‰:

```json
{{
  "action": "BUY" or "SELL" or "HOLD",
  "confidence": 50-85ã®ç¯„å›²ã‚’ç›®å®‰ï¼ˆå®Œç’§ã§ãªãã¦ã‚‚ã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼‰,
  "reasoning": "åˆ¤æ–­ç†ç”±ï¼ˆM15ã®çŠ¶æ³ã‚’ä¸­å¿ƒã«ã€ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ ¹æ‹ ã‚’æ˜ç¢ºã«ï¼‰",
  "entry_price": ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ¨å¥¨ä¾¡æ ¼ï¼ˆç¾åœ¨ä¾¡æ ¼ä»˜è¿‘ï¼‰,
  "stop_loss": ã‚¹ãƒˆãƒƒãƒ—ãƒ­ã‚¹æ¨å¥¨ä¾¡æ ¼ï¼ˆ10-15pipsï¼‰,
  "take_profit": ãƒ†ã‚¤ã‚¯ãƒ—ãƒ­ãƒ•ã‚£ãƒƒãƒˆæ¨å¥¨ä¾¡æ ¼ï¼ˆ15-30pipsã€ãƒªã‚¹ã‚¯ãƒªãƒ¯ãƒ¼ãƒ‰1:1.5ä»¥ä¸Šï¼‰
}}
```
"""
        return prompt

    def _parse_response(self, response_text: str) -> Dict:
        """
        AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹

        Args:
            response_text: AIã‹ã‚‰ã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸåˆ¤æ–­çµæœã®è¾æ›¸
        """
        try:
            # JSONãƒ–ãƒ­ãƒƒã‚¯ï¼ˆ```json ... ```ï¼‰ã‚’æŠ½å‡º
            json_match = re.search(
                r'```json\s*(\{.*?\})\s*```',
                response_text,
                re.DOTALL
            )

            if json_match:
                json_text = json_match.group(1)
            else:
                # JSONãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆã€{ } ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’æ¢ã™
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(0)
                else:
                    # JSONå½¢å¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
                    raise ValueError("No JSON format found in response")

            # JSONã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(json_text)

            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼
            if 'action' not in result:
                raise ValueError("'action' field is missing in response")

            if result['action'] not in ['BUY', 'SELL', 'HOLD']:
                raise ValueError(f"Invalid action: {result['action']}")

            # confidenceã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            if 'confidence' not in result:
                result['confidence'] = 50

            # reasoningã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            if 'reasoning' not in result:
                result['reasoning'] = 'No reasoning provided'

            return result

        except (json.JSONDecodeError, ValueError) as e:
            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            self.logger.error(f"Failed to parse AI response: {e}")
            self.logger.debug(f"Response text: {response_text}")

            return {
                'action': 'HOLD',
                'confidence': 0,
                'reasoning': f'Failed to parse AI response: {str(e)}'
            }
