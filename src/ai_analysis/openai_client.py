"""
========================================
OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
========================================

ãƒ•ã‚¡ã‚¤ãƒ«å: openai_client.py
ãƒ‘ã‚¹: src/ai_analysis/openai_client.py

ã€æ¦‚è¦ã€‘
OpenAI ChatGPT APIã‚’ä½¿ç”¨ã—ã¦LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã™ã€‚
BaseLLMClientã‚’ç¶™æ‰¿ã—ã€çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

ã€å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‘
- gpt-4o
- gpt-4o-mini
- gpt-4-turbo
- gpt-3.5-turbo
- o1-preview
- o1-mini

æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§: https://platform.openai.com/docs/models

ã€ä½¿ç”¨ä¾‹ã€‘
```python
from src.ai_analysis.openai_client import OpenAIClient

client = OpenAIClient(api_key="your_api_key")
response = client.generate_response(
    prompt="Analyze this market data...",
    model="gpt-4o",
    temperature=0.3,
    max_tokens=2000
)
```

ã€ä½œæˆæ—¥ã€‘2025-10-23
"""

from typing import Optional
import logging
import time
from openai import OpenAI, InternalServerError, RateLimitError
from src.ai_analysis.base_llm_client import BaseLLMClient


class OpenAIClient(BaseLLMClient):
    """
    OpenAI ChatGPT APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

    OpenAI APIã‚’ä½¿ç”¨ã—ã¦LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """

    def __init__(self, api_key: str):
        """
        OpenAIClientã®åˆæœŸåŒ–

        Args:
            api_key: OpenAI APIã‚­ãƒ¼
        """
        super().__init__(api_key)
        self.client = OpenAI(api_key=api_key)
        self.logger.info("OpenAI client initialized")

    def _select_model(self, model: str) -> str:
        """
        ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹

        Args:
            model: ãƒ¢ãƒ‡ãƒ«åï¼ˆå®Œå…¨ãªãƒ¢ãƒ‡ãƒ«å ä¾‹: gpt-4oï¼‰
                ã¾ãŸã¯çŸ­ç¸®åï¼ˆPhaseåï¼‰:
                - 'daily_analysis': MODEL_DAILY_ANALYSISã®å€¤ã‚’ä½¿ç”¨
                - 'periodic_update': MODEL_PERIODIC_UPDATEã®å€¤ã‚’ä½¿ç”¨
                - 'position_monitor': MODEL_POSITION_MONITORã®å€¤ã‚’ä½¿ç”¨
                - 'emergency_evaluation': MODEL_EMERGENCY_EVALUATIONã®å€¤ã‚’ä½¿ç”¨

        Returns:
            str: å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å

        Raises:
            ValueError: ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒä¸æ­£ãªå ´åˆ
        """
        from src.utils.config import get_config
        config = get_config()

        # Phaseåã‹ã‚‰.envè¨­å®šã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        phase_to_config_mapping = {
            'daily_analysis': config.model_daily_analysis,
            'periodic_update': config.model_periodic_update,
            'position_monitor': config.model_position_monitor,
            'emergency_evaluation': config.model_emergency_evaluation,
        }

        # Phaseåã®å ´åˆã¯.envã‹ã‚‰è¨­å®šã‚’å–å¾—
        if model in phase_to_config_mapping:
            model_name = phase_to_config_mapping[model]
            if not model_name:
                raise ValueError(
                    f"Model for phase '{model}' is not configured in .env file. "
                    f"Please set the appropriate MODEL_* environment variable."
                )
            self.logger.debug(f"Phase '{model}' mapped to model '{model_name}'")
        else:
            # ã™ã§ã«å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«å
            model_name = model

        # ãƒ¢ãƒ‡ãƒ«åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ - OpenAIClient ã¯OpenAIãƒ¢ãƒ‡ãƒ«ã®ã¿å¯¾å¿œ
        if not model_name.startswith(('gpt-', 'o1-', 'chatgpt-')):
            # OpenAIä»¥å¤–ã®ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
            provider_hint = "Unknown"
            if model_name.startswith('gemini-'):
                provider_hint = "Google Gemini"
            elif model_name.startswith('claude-'):
                provider_hint = "Anthropic Claude"

            raise ValueError(
                f"OpenAIClient cannot use non-OpenAI model: '{model_name}' ({provider_hint})\n"
                f"Please configure an OpenAI model (gpt-*, o1-*, chatgpt-*) in your .env file.\n"
                f"Example OpenAI models:\n"
                f"  - gpt-4o\n"
                f"  - gpt-4o-mini\n"
                f"  - gpt-5-nano\n"
                f"  - o1-preview\n"
                f"\n"
                f"If you want to use {provider_hint} models, configure them in MODEL_* variables\n"
                f"and the system will automatically select the appropriate client."
            )

        return model_name

    def generate_response(
        self,
        prompt: str,
        model: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        OpenAI APIã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ

        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
            model: ãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: gpt-4o, gpt-4o-miniï¼‰
                  ã¾ãŸã¯Phaseåï¼ˆä¾‹: daily_analysis, periodic_updateï¼‰
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0.0-2.0ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.0ï¼‰
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtop_p, frequency_penalty, etc.ï¼‰

        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ

        Raises:
            Exception: APIå‘¼ã³å‡ºã—ãŒå¤±æ•—ã—ãŸå ´åˆ
        """
        try:
            # Phaseåã‚’å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«åã«å¤‰æ›
            actual_model = self._select_model(model)

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
            params = {
                "model": actual_model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
            }

            # GPT-5ã‚„o1ã‚·ãƒªãƒ¼ã‚ºã‹ã©ã†ã‹ã‚’åˆ¤å®š
            is_new_model = actual_model.startswith(('gpt-5', 'o1-', 'o3-'))

            # temperatureã®è¨­å®šï¼ˆæ–°ãƒ¢ãƒ‡ãƒ«ã¯éå¯¾å¿œï¼‰
            if temperature is not None and not is_new_model:
                params["temperature"] = temperature

            # max_tokensã®è¨­å®šï¼ˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ä½¿ã„åˆ†ã‘ï¼‰
            if max_tokens is not None:
                if is_new_model:
                    # GPT-5ã‚„o1ã‚·ãƒªãƒ¼ã‚ºã¯ max_completion_tokens ã‚’ä½¿ç”¨
                    params["max_completion_tokens"] = max_tokens
                else:
                    # å¾“æ¥ã®GPT-4ã€GPT-3.5ãªã©ã¯ max_tokens ã‚’ä½¿ç”¨
                    params["max_tokens"] = max_tokens

            # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆphaseé™¤å¤–ï¼‰
            phase = kwargs.pop('phase', 'Unknown')
            params.update(kwargs)

            # ãƒ­ã‚°å‡ºåŠ›ï¼ˆå®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼‰
            token_param = "max_completion_tokens" if is_new_model else "max_tokens"
            self.logger.debug(
                f"OpenAI API request: model={actual_model}, "
                f"temperature={temperature if not is_new_model else 'N/A'}, "
                f"{token_param}={max_tokens}"
            )

            # APIå‘¼ã³å‡ºã—ï¼ˆãƒªãƒˆãƒ©ã‚¤å‡¦ç†ä»˜ãï¼‰
            max_retries = 3
            retry_delay = 2  # åˆå›å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

            for attempt in range(max_retries):
                try:
                    response = self.client.chat.completions.create(**params)
                    break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹

                except (InternalServerError, RateLimitError) as e:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•: 2ç§’ã€4ç§’ã€8ç§’
                        self.logger.warning(
                            f"OpenAI API error (attempt {attempt + 1}/{max_retries}): {e}. "
                            f"Retrying in {wait_time} seconds..."
                        )
                        time.sleep(wait_time)
                    else:
                        # æœ€å¾Œã®ãƒªãƒˆãƒ©ã‚¤ã‚‚å¤±æ•—
                        self.logger.error(f"OpenAI API failed after {max_retries} attempts: {e}")
                        raise

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.choices:
                raise ValueError("OpenAI API returned no choices")

            text = response.choices[0].message.content

            # finish_reasonã‚’ãƒã‚§ãƒƒã‚¯
            finish_reason = response.choices[0].finish_reason
            if finish_reason == "length":
                self.logger.warning(
                    f"Response was truncated due to max_tokens limit. "
                    f"Current max_tokens: {max_tokens}. "
                    f"Consider increasing max_tokens in .env"
                )
            elif finish_reason == "content_filter":
                raise ValueError(
                    "Response was filtered by OpenAI content policy. "
                    "Please modify your prompt."
                )

            self.logger.debug(
                f"OpenAI API response received: "
                f"finish_reason={finish_reason}, "
                f"length={len(text)} chars"
            )

            # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²
            if hasattr(response, 'usage'):
                from src.ai_analysis.token_usage_tracker import get_token_tracker
                tracker = get_token_tracker()
                tracker.record_usage(
                    phase=phase,
                    provider='openai',
                    model=actual_model,
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens
                )

            return text

        except Exception as e:
            self.logger.error(f"OpenAI API error: {e}")
            raise

    def test_connection(self, verbose: bool = False, model: Optional[str] = None) -> bool:
        """
        OpenAI APIã¸ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ

        Args:
            verbose: è©³ç´°ãªãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
            model: ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ï¼‰

        Returns:
            bool: True=æ¥ç¶šæˆåŠŸ, False=æ¥ç¶šå¤±æ•—
        """
        try:
            if verbose:
                print("ğŸ”Œ OpenAI APIæ¥ç¶šãƒ†ã‚¹ãƒˆä¸­...", end='', flush=True)

            # ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆæœ€ã‚‚å®‰ä¾¡ï¼‰ã‚’ä½¿ç”¨
            test_model = model if model else "gpt-3.5-turbo"

            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡
            test_prompt = "Hello, this is a connection test. Please respond with 'OK'."
            response = self.generate_response(
                prompt=test_prompt,
                model=test_model,
                max_tokens=10,
                phase="Connection Test"  # ãƒ¬ãƒãƒ¼ãƒˆã§è­˜åˆ¥ã§ãã‚‹ã‚ˆã†ã«phaseã‚’è¨­å®š
            )

            if response:
                if verbose:
                    print(" âœ“ æ¥ç¶šæˆåŠŸ")
                self.logger.info("OpenAI API connection test: SUCCESS")
                return True
            else:
                if verbose:
                    print(" âœ— æ¥ç¶šå¤±æ•—")
                self.logger.error("OpenAI API connection test: FAILED (empty response)")
                return False

        except Exception as e:
            if verbose:
                print(f" âœ— æ¥ç¶šå¤±æ•—: {e}")
            self.logger.error(f"OpenAI API connection test: FAILED - {e}")
            return False

    def get_provider_name(self) -> str:
        """
        ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã‚’å–å¾—

        Returns:
            str: "openai"
        """
        return "openai"
